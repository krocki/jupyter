{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c9919944-b79a-49e1-8c75-c28316ffa9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params\n",
      "-----\n",
      "embed      :      1585152\n",
      "attn_qkv   :       786432\n",
      "attn_mask  :            0\n",
      "attn_proj  :       262144\n",
      "ff         :      2097152\n",
      "de-embed   :            0\n",
      "dec_head   :      1552384\n",
      "params (G) :     0.006283\n",
      "\n",
      "flops\n",
      "-----\n",
      "embed      :         1024\n",
      "attn_qkv   :      1572864\n",
      "attn_mask  :       262144\n",
      "attn_proj  :       524288\n",
      "ff         :      4194304\n",
      "de-embed   :      3104768\n",
      "total GF   :     0.009659\n",
      "\n",
      "C_forward=6553600\n"
     ]
    }
   ],
   "source": [
    "# Flops in Transformer\n",
    "# per:\n",
    "# Scaling Laws for Neural Language Models\n",
    "# https://arxiv.org/pdf/2001.08361.pdf\n",
    "\n",
    "n_layer      = 4     # number of layers\n",
    "d_model      = 256   # dimension of residual stream\n",
    "d_ff         = 4 * d_model   # dimension of intermediate feed-forward layer\n",
    "d_attn       = 256   # dimension of the attention output\n",
    "n_heads      = 4     # number of attention heads per layer\n",
    "n_ctx        = 128   # number of context input tokens\n",
    "n_vocab      = 6064  # vocab size\n",
    "d_embd       = 256   # embedding size\n",
    "\n",
    "# GPT-1\n",
    "#n_layer      = 12    # number of layers\n",
    "#d_model      = 768   # dimension of residual stream\n",
    "#d_ff         = 3072  # dimension of intermediate feed-forward layer\n",
    "#d_attn       = 768   # dimension of the attention output\n",
    "#n_heads      = 12    # number of attention heads per layer\n",
    "#n_ctx        = 256   # number of context input tokens\n",
    "#n_vocab      = 40000 # vocab size\n",
    "#d_embd       = 768   # embedding size\n",
    "\n",
    "# GPT-2\n",
    "#n_layer      = 48    # number of layers\n",
    "#d_model      = 1600  # dimension of residual stream\n",
    "#d_ff         = 3072  # dimension of intermediate feed-forward layer\n",
    "#d_attn       = 1600  # dimension of the attention output\n",
    "#n_heads      = 48    # number of attention heads per layer\n",
    "#n_ctx        = 1024  # number of context input tokens\n",
    "#n_vocab      = 50257 # vocab size\n",
    "#d_embd       = 1600  # embedding size\n",
    "\n",
    "# GPT-3\n",
    "#n_layer      = 96    # number of layers\n",
    "#d_model      = 12288 # dimension of residual stream\n",
    "#d_ff         = 4 * d_model  # dimension of intermediate feed-forward layer\n",
    "#d_attn       = 12288 # dimension of the attention output\n",
    "#n_heads      = 96    # number of attention heads per layer\n",
    "#n_ctx        = 2048  # number of context input tokens\n",
    "#n_vocab      = 50257 # vocab size\n",
    "#d_embd       = 12288 # embedding size\n",
    "\n",
    "#         self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "#         self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "#         self.attn = CausalSelfAttention(config)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "#             nn.Dropout(config.resid_pdrop),\n",
    "#         )\n",
    "        \n",
    "# attn k 256 * 256\n",
    "# attn q 256 * 256\n",
    "# attn v 256 * 256\n",
    "# attn p 256 * 256\n",
    "# mlp 0 - 4 * 256 * 256\n",
    "# mlp 2 - 4 * 256 * 256\n",
    "\n",
    "params = {\n",
    "    'embed':     (n_vocab + n_ctx) * d_model,\n",
    "    'attn_qkv':  n_layer * d_model * 3 * d_attn,\n",
    "    'attn_mask': 0,\n",
    "    'attn_proj': n_layer * d_model * d_attn,\n",
    "    'ff':        n_layer * 2 * d_model * d_ff,\n",
    "    'de-embed':  0,\n",
    "    'dec_head':  (n_vocab) * d_model\n",
    "}\n",
    "\n",
    "# FLOPs per token\n",
    "flops = {\n",
    "    'embed':     4 * d_model,\n",
    "    'attn_qkv':  2 * n_layer * d_model * 3 * d_attn,\n",
    "    'attn_mask': 2 * n_layer * n_ctx * d_attn,\n",
    "    'attn_proj': 2 * n_layer * d_attn * d_embd,\n",
    "    'ff':        2 * n_layer * 2 * d_model * d_ff,\n",
    "    'de-embed':  2 * d_model * n_vocab,\n",
    "}\n",
    "\n",
    "\n",
    "total_params = 0\n",
    "print('params\\n-----')\n",
    "for k in params:\n",
    "    print(f'{k:11s}: {params[k]:12d}')\n",
    "    total_params += params[k]\n",
    "print(f'params (G) : {total_params / (1e9) :12.6f}')\n",
    "\n",
    "total_flops = 0\n",
    "print('\\nflops\\n-----')\n",
    "for k in flops:\n",
    "    print(f'{k:11s}: {flops[k]:12d}')\n",
    "    total_flops += flops[k]\n",
    "\n",
    "print(f'total GF   : {total_flops / (1e9) :12.6f}')\n",
    "print('')\n",
    "\n",
    "#  the number of model parameters, excluding all vocabulary and positional embeddings\n",
    "N = 2 * d_model * n_layer * (2 * d_attn + d_ff)\n",
    "# total, non-embedding\n",
    "C_forward = 2 * N + 2 * n_layer * n_ctx * d_model # 2.2\n",
    "print(f'{C_forward=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e80d61-65b8-477b-a892-e60289853af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
