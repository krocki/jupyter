{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e285adc4-d298-42cc-9f6d-6b4c178b5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A (GPU):\n",
      "[[ 0.83662224  1.0413324  -0.99847525  0.998356  ]\n",
      " [ 0.61707264  0.36569452  0.7032135  -0.7053676 ]\n",
      " [ 0.21370357 -1.2686112  -0.47671828 -0.96448565]\n",
      " [-0.7209121   0.33701676 -0.99147284 -0.22911051]]\n",
      "Matrix B (GPU):\n",
      "[[ 1.332438   -1.1402535  -0.5546992  -0.19572684]\n",
      " [-0.8257529  -1.0863628   0.33289486 -0.31577232]\n",
      " [-1.0514492  -0.9113693  -0.1734687  -0.44951436]\n",
      " [ 0.519137    0.29076    -0.751766   -0.00241711]]\n",
      "Matrix C (GPU):\n",
      "[[ 1.8229936  -0.8849646  -0.69474536 -0.04615755]\n",
      " [-0.58533806 -1.9468762   0.18773401 -0.5506535 ]\n",
      " [ 1.3328509   1.2885283   0.26690787  0.5753877 ]\n",
      " [-0.3153197   1.2928818   0.8563075   0.48091635]]\n",
      "CPU-GPU difference:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Multiplies two square matrices together using a *single* block of threads and \n",
    "global memory only. Each thread computes one element of the resulting matrix.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "kernel_code_template = \"\"\"\n",
    "__global__ void MatrixMulKernel(float *a, float *b, float *c)\n",
    "{\n",
    "    // 2D Thread ID (assuming that only *one* block will be executed)\n",
    "    int tx = threadIdx.x;\n",
    "    int ty = threadIdx.y;\n",
    "\n",
    "    // Pvalue is used to store the element of the matrix\n",
    "    // that is computed by the thread\n",
    "    float Pvalue = 0;\n",
    "\n",
    "    // Each thread loads one row of M and one column of N, \n",
    "    //   to produce one element of P.\n",
    "    for (int k = 0; k < %(MATRIX_SIZE)s; ++k) {\n",
    "        float Aelement = a[ty * %(MATRIX_SIZE)s + k];\n",
    "        float Belement = b[k * %(MATRIX_SIZE)s + tx];\n",
    "        Pvalue += Aelement * Belement;\n",
    "    }\n",
    "\n",
    "    // Write the matrix to device memory;\n",
    "    // each thread writes one element\n",
    "    c[ty * %(MATRIX_SIZE)s + tx] = Pvalue;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# define the (square) matrix size\n",
    "#  note that we'll only use *one* block of threads here\n",
    "#  as a consequence this number (squared) can't exceed max_threads,\n",
    "#  see http://documen.tician.de/pycuda/util.html#pycuda.tools.DeviceData\n",
    "#  for more information on how to get this number for your device\n",
    "MATRIX_SIZE = 4\n",
    "\n",
    "# create two random square matrices\n",
    "a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "\n",
    "# compute reference on the CPU to verify GPU computation\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "\n",
    "# transfer host (CPU) memory to device (GPU) memory \n",
    "a_gpu = gpuarray.to_gpu(a_cpu) \n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "# create empty gpu array for the result (C = A * B)\n",
    "c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32)\n",
    "\n",
    "# get the kernel code from the template \n",
    "# by specifying the constant MATRIX_SIZE\n",
    "kernel_code = kernel_code_template % {\n",
    "    'MATRIX_SIZE': MATRIX_SIZE \n",
    "    }\n",
    "\n",
    "# compile the kernel code \n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "matrixmul = mod.get_function(\"MatrixMulKernel\")\n",
    "\n",
    "# call the kernel on the card\n",
    "matrixmul(\n",
    "    # inputs\n",
    "    a_gpu, b_gpu, \n",
    "    # output\n",
    "    c_gpu, \n",
    "    # (only one) block of MATRIX_SIZE x MATRIX_SIZE threads\n",
    "    block = (MATRIX_SIZE, MATRIX_SIZE, 1),\n",
    "    )\n",
    "\n",
    "# print the results\n",
    "\n",
    "print(\"Matrix A (GPU):\")\n",
    "print(a_gpu.get())\n",
    "\n",
    "\n",
    "print(\"Matrix B (GPU):\")\n",
    "print(b_gpu.get())\n",
    "\n",
    "\n",
    "print(\"Matrix C (GPU):\")\n",
    "print(c_gpu.get())\n",
    "\n",
    "\n",
    "print(\"CPU-GPU difference:\")\n",
    "print(c_cpu - c_gpu.get())\n",
    "\n",
    "np.allclose(c_cpu, c_gpu.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1329b-b69c-49e9-aee0-8d30e55ef7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
