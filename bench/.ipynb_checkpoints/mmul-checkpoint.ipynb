{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532a46c6-a0fe-4394-b258-d02328e207a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackwell,  07/01/2022 15:05:52\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "host = os.uname()[1]\n",
    "dt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "print(f\"{host}, \", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe9a28a-705d-4f58-8607-7745eb979822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libcublas.so\n",
      "<CDLL '/home/krocki/nv/cuBLAS/build/src/libcublas.so', handle 2e43ab0 at 0x7f17e1b5bac0>\n",
      "<CDLL '/home/krocki/nv/cuBLAS/build/src/libcublas.so', handle 2e43ab0 at 0x7f17e1b5bac0>\n",
      "11901\n",
      "\n",
      "pycuda.VERSION_TEXT='2021.1'\n",
      "drv.get_version()=(11, 5, 0)\n",
      "drv.get_driver_version()=11050\n",
      "\n",
      "Device #1: NVIDIA A100-SXM-80GB\n",
      "  Arch: 8.0, Mem: 81251 MB, 2039 GBps\n",
      "  108 SMs, 6912 cores, 40.0 MB L2, 1.41 GHz\n",
      "     TF/s:      9.75 FP64     19.49 FP32     77.97 FP16\n",
      "  TC TF/s:     19.49 FP64    155.93 FP32    311.87 FP16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import pycuda.gpuarray as gpuarray\n",
    "import skcuda.cublas as cublas\n",
    "\n",
    "import cuda_util\n",
    "\n",
    "print()\n",
    "print(f'{pycuda.VERSION_TEXT=}')\n",
    "print(f'{drv.get_version()=}')\n",
    "print(f'{drv.get_driver_version()=}')\n",
    "print()\n",
    "\n",
    "dev_id = 1\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "    \n",
    "ctx = drv.Device(dev_id).make_context()\n",
    "dev = drv.Context.get_device()\n",
    "cuda_util.query_dev(drv, dev_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32b4ca3-749c-43d6-86e0-23e2443aba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmul(handle, M, N, K, repeat, dtype):\n",
    "\n",
    "    A = np.array(np.random.rand(M, K), dtype=dtype, order='F').astype(dtype)\n",
    "    B = np.array(np.random.rand(K, N), dtype=dtype, order='F').astype(dtype)\n",
    "    \n",
    "    #A = np.ones((M, K), dtype=dtype, order='F')\n",
    "    #B = np.ones((K, N), dtype=dtype, order='F')\n",
    "    \n",
    "    #A = np.zeros((M, K), dtype=dtype, order='F')\n",
    "    #B = np.zeros((K, N), dtype=dtype, order='F')\n",
    "    \n",
    "    d_A = gpuarray.to_gpu(A)\n",
    "    d_B = gpuarray.to_gpu(B)\n",
    "\n",
    "    m, k = d_A.shape\n",
    "    k, n = d_B.shape\n",
    "\n",
    "    d_C = gpuarray.empty((m, n), dtype=dtype, order='F')\n",
    "\n",
    "    alpha = dtype(1)\n",
    "    beta  = dtype(0)\n",
    "    \n",
    "    start = drv.Event()\n",
    "    end   = drv.Event()\n",
    "\n",
    "    # cublas<T>gemm(handle,\n",
    "    # transa, transb, m, n, k, alpha,\n",
    "    # A, lda,\n",
    "    # B, ldb, beta,\n",
    "    # C, ldc)\n",
    "    \n",
    "    xgemm = {\n",
    "        np.float32: cublas.cublasSgemm,\n",
    "        np.float64: cublas.cublasDgemm\n",
    "    }\n",
    "    \n",
    "    gemm_gflop = 1e-9 * m * n * k * 2\n",
    "    \n",
    "    start.record()\n",
    "    for i in range(repeat):\n",
    "        xgemm[dtype](handle,\n",
    "                     transa='n', transb='n',\n",
    "                     m=m, n=n, k=k, alpha=alpha,\n",
    "                     A=d_A.ptr, lda=m,\n",
    "                     B=d_B.ptr, ldb=k, beta=beta,\n",
    "                     C=d_C.ptr, ldc=m)\n",
    "        \n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    gemm_time = end.time_since(start) / (repeat * 1e3)\n",
    "\n",
    "    d_C = d_C.reshape(d_C.shape, order = 'F')\n",
    "\n",
    "    C = np.dot(A, B).astype(dtype)\n",
    "\n",
    "    err = np.linalg.norm(d_C.get() - C)\n",
    "\n",
    "    gflop_per_sec = gemm_gflop/gemm_time\n",
    "    \n",
    "    print(f'{err=:12g}, {m=:5}, {n=:5}, {k=:5}'\n",
    "          f', time={gemm_time:.4f} ms'\n",
    "          f', {gflop_per_sec:10.3f} GF/s')\n",
    "    \n",
    "    return gflop_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e006cb-a97e-4864-a88b-0e59613ac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test tensor core math\n",
    "\"\"\"\n",
    "\n",
    "import ctypes\n",
    "\n",
    "\"\"\"\n",
    "/*Enum for default math mode/tensor operation*/\n",
    "typedef enum {\n",
    "  CUBLAS_DEFAULT_MATH = 0,\n",
    "\n",
    "  /* deprecated, same effect as using CUBLAS_COMPUTE_32F_FAST_16F, will be removed in a future release */\n",
    "  CUBLAS_TENSOR_OP_MATH = 1,\n",
    "\n",
    "  /* same as using matching _PEDANTIC compute type when using cublas<T>routine calls or cublasEx() calls with\n",
    "     cudaDataType as compute type */\n",
    "  CUBLAS_PEDANTIC_MATH = 2,\n",
    "\n",
    "  /* allow accelerating single precision routines using TF32 tensor cores */\n",
    "  CUBLAS_TF32_TENSOR_OP_MATH = 3,\n",
    "\n",
    "  /* flag to force any reductons to use the accumulator type and not output type in case of mixed precision routines\n",
    "     with lower size output type */\n",
    "  CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION = 16,\n",
    "} cublasMath_t;\n",
    "\n",
    "typedef enum {\n",
    "  CUBLAS_COMPUTE_16F = 64,           /* half - default */\n",
    "  CUBLAS_COMPUTE_16F_PEDANTIC = 65,  /* half - pedantic */\n",
    "  CUBLAS_COMPUTE_32F = 68,           /* float - default */\n",
    "  CUBLAS_COMPUTE_32F_PEDANTIC = 69,  /* float - pedantic */\n",
    "  CUBLAS_COMPUTE_32F_FAST_16F = 74,  /* float - fast, allows down-converting inputs to half or TF32 */\n",
    "  CUBLAS_COMPUTE_32F_FAST_16BF = 75, /* float - fast, allows down-converting inputs to bfloat16 or TF32 */\n",
    "  CUBLAS_COMPUTE_32F_FAST_TF32 = 77, /* float - fast, allows down-converting inputs to TF32 */\n",
    "  CUBLAS_COMPUTE_64F = 70,           /* double - default */\n",
    "  CUBLAS_COMPUTE_64F_PEDANTIC = 71,  /* double - pedantic */\n",
    "  CUBLAS_COMPUTE_32I = 72,           /* signed 32-bit int - default */\n",
    "  CUBLAS_COMPUTE_32I_PEDANTIC = 73,  /* signed 32-bit int - pedantic */\n",
    "} cublasComputeType_t;\n",
    "\"\"\"\n",
    "CUBLAS_MATH_MODE = {\n",
    "    'CUBLAS_DEFAULT_MATH': 0,\n",
    "    'CUBLAS_TENSOR_OP_MATH': 1,\n",
    "    'CUBLAS_PEDANTIC_MATH': 2,\n",
    "    'CUBLAS_TF32_TENSOR_OP_MATH': 3,\n",
    "    'CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION': 16\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "cublasStatus_t cublasSetMathMode(cublasHandle_t handle, cublasMath_t mode)\n",
    "\"\"\"\n",
    "def cublasSetMathMode(handle, lib, mode):\n",
    "    status = lib._libcublas.cublasSetMathMode(handle, mode)\n",
    "    lib.cublasCheckStatus(status)\n",
    "\n",
    "\"\"\"\n",
    "cublasStatus_t cublasGetMathMode(cublasHandle_t handle, cublasMath_t *mode)\n",
    "\"\"\"\n",
    "def cublasGetMathMode(handle, lib):\n",
    "    mode = ctypes.c_int()\n",
    "    status = lib._libcublas.cublasGetMathMode(handle, ctypes.byref(mode))\n",
    "    lib.cublasCheckStatus(status)\n",
    "    return mode.value\n",
    "\n",
    "\"\"\"\n",
    "cublasStatus_t cublasLoggerConfigure(\n",
    "    int             logIsOn,\n",
    "    int             logToStdOut,\n",
    "    int             logToStdErr,\n",
    "    const char*     logFileName)\n",
    "\"\"\"\n",
    "def cublasLoggerConfigure(lib, logIsOn, logToStdOut, logToStdErr, logFileName):\n",
    "    status = lib._libcublas.cublasGetMathMode(logIsOn, logToStdOut, logToStdErr, logFileName)\n",
    "    lib.cublasCheckStatus(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037fd249-521a-42ff-b68c-4bea346e0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "# square\n",
    "sizes = [64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "precisions = [np.float64, np.float32]\n",
    "cublas_modes = [\n",
    "             'CUBLAS_DEFAULT_MATH',\n",
    "             'CUBLAS_TENSOR_OP_MATH',\n",
    "             'CUBLAS_PEDANTIC_MATH',\n",
    "             'CUBLAS_TF32_TENSOR_OP_MATH',\n",
    "             'CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3b91c-c983-458c-b306-ea8cdd03c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dtype=<class 'numpy.float64'>\n",
      "-----------\n",
      "mode='CUBLAS_DEFAULT_MATH', cublasGetMathMode(handle, cublas)=0\n",
      "\n",
      "err= 2.39149e-13, m=   64, n=   64, k=   64, time=0.0000 ms,     14.423 GF/s\n",
      "err=           0, m=  128, n=  128, k=  128, time=0.0000 ms,    349.153 GF/s\n",
      "err=           0, m=  256, n=  256, k=  256, time=0.0000 ms,   1343.123 GF/s\n",
      "err= 3.95789e-11, m=  512, n=  512, k=  512, time=0.0000 ms,   8758.204 GF/s\n",
      "err= 2.17734e-10, m= 1024, n= 1024, k= 1024, time=0.0001 ms,  14772.577 GF/s\n",
      "err= 1.19272e-09, m= 2048, n= 2048, k= 2048, time=0.0010 ms,  16631.740 GF/s\n",
      "err= 6.63736e-09, m= 4096, n= 4096, k= 4096, time=0.0081 ms,  16920.425 GF/s\n",
      "err= 3.72743e-08, m= 8192, n= 8192, k= 8192, time=0.0676 ms,  16276.475 GF/s\n",
      "mode='CUBLAS_TENSOR_OP_MATH', cublasGetMathMode(handle, cublas)=1\n",
      "\n",
      "err= 2.35486e-13, m=   64, n=   64, k=   64, time=0.0000 ms,     19.845 GF/s\n",
      "err=           0, m=  128, n=  128, k=  128, time=0.0000 ms,    615.073 GF/s\n",
      "err=           0, m=  256, n=  256, k=  256, time=0.0000 ms,   3857.896 GF/s\n",
      "err= 3.95618e-11, m=  512, n=  512, k=  512, time=0.0000 ms,  10665.745 GF/s\n",
      "err= 2.18467e-10, m= 1024, n= 1024, k= 1024, time=0.0001 ms,  15096.928 GF/s\n",
      "err= 1.19329e-09, m= 2048, n= 2048, k= 2048, time=0.0010 ms,  16620.413 GF/s\n",
      "err= 6.64014e-09, m= 4096, n= 4096, k= 4096, time=0.0081 ms,  16950.870 GF/s\n"
     ]
    }
   ],
   "source": [
    "for dtype in precisions:\n",
    "    results[dtype] = {}\n",
    "    print(f'\\n{dtype=}\\n-----------')\n",
    "    for mode in cublas_modes:\n",
    "\n",
    "        handle = cublas.cublasCreate()\n",
    "        cublasSetMathMode(handle, cublas, CUBLAS_MATH_MODE[mode])\n",
    "        print(f'{mode=}, {cublasGetMathMode(handle, cublas)=}\\n')\n",
    "\n",
    "        results[dtype][mode] = []\n",
    "        for i in sizes:\n",
    "            M = N = K = i\n",
    "            flops = mmul(handle, M, N, K, repeat=10, dtype=dtype)\n",
    "            results[dtype][mode].append(flops)\n",
    "\n",
    "        cublas.cublasDestroy(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f0bae-47dc-43ef-9b38-881301d51a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "results_1 = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7c916-6610-45ca-8de5-19438cb78b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randn\n",
    "for dtype in precisions:\n",
    "    results[dtype] = {}\n",
    "    print(f'\\n{dtype=}\\n-----------')\n",
    "    for mode in cublas_modes:\n",
    "\n",
    "        handle = cublas.cublasCreate()\n",
    "        cublasSetMathMode(handle, cublas, CUBLAS_MATH_MODE[mode])\n",
    "        print(f'{mode=}, {cublasGetMathMode(handle, cublas)=}\\n')\n",
    "\n",
    "        results[dtype][mode] = []\n",
    "        for i in sizes:\n",
    "            M = N = K = i\n",
    "            flops = mmul(handle, M, N, K, repeat=10, dtype=dtype)\n",
    "            results[dtype][mode].append(flops)\n",
    "\n",
    "        cublas.cublasDestroy(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9fe47-e835-4efe-bd24-5a1c6a6b9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros\n",
    "for dtype in precisions:\n",
    "    results[dtype] = {}\n",
    "    print(f'\\n{dtype=}\\n-----------')\n",
    "    for mode in cublas_modes:\n",
    "\n",
    "        handle = cublas.cublasCreate()\n",
    "        cublasSetMathMode(handle, cublas, CUBLAS_MATH_MODE[mode])\n",
    "        print(f'{mode=}, {cublasGetMathMode(handle, cublas)=}\\n')\n",
    "\n",
    "        results[dtype][mode] = []\n",
    "        for i in sizes:\n",
    "            M = N = K = i\n",
    "            flops = mmul(handle, M, N, K, repeat=10, dtype=dtype)\n",
    "            results[dtype][mode].append(flops)\n",
    "\n",
    "        cublas.cublasDestroy(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27131d-e5ce-48b4-8da6-69c97cee0a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
