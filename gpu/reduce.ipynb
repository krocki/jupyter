{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95736f23-234e-4984-9039-2a4245aae467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM-80GB: 108 SMs, 2039 GBps, 40.0 MB L2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pycuda.driver as drv\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda.autoinit import context, device\n",
    "\n",
    "file_path = os.getcwd()\n",
    "attributes = drv.Context.get_device().get_attributes()\n",
    "sms = attributes[drv.device_attribute.MULTIPROCESSOR_COUNT]\n",
    "mcr = attributes[drv.device_attribute.MEMORY_CLOCK_RATE]\n",
    "bus = attributes[drv.device_attribute.GLOBAL_MEMORY_BUS_WIDTH]\n",
    "l2s = attributes[drv.device_attribute.L2_CACHE_SIZE]\n",
    "\n",
    "print(f\"{device.name()}: {sms} SMs, {mcr*1000*bus*2*1e-9/8:.0f} GBps, {l2s/1024**2:.1f} MB L2\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4fdeb8-9d62-4201-a7cf-03b121ae35cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBps: 636 ms:  0.211 dim: (       1,33554432) axis:1 max_err:    0.001 max_val: 3475.572 l2_err: 0.00000\n",
      "GBps: 816 ms:  0.329 dim: (       1,33554432) axis:0 max_err:    0.000 max_val:    1.000 l2_err: 0.00000\n",
      "GBps: 792 ms:  0.169 dim: (       2,16777216) axis:1 max_err:    0.000 max_val: -218.910 l2_err: 0.00000\n",
      "GBps:1050 ms:  0.192 dim: (       2,16777216) axis:0 max_err:    0.000 max_val:    1.999 l2_err: 0.00000\n",
      "GBps: 876 ms:  0.153 dim: (       4, 8388608) axis:1 max_err:    0.000 max_val: 1303.801 l2_err: 0.00000\n",
      "GBps:1538 ms:  0.109 dim: (       4, 8388608) axis:0 max_err:    0.000 max_val:    3.972 l2_err: 0.00000\n",
      "GBps: 853 ms:  0.157 dim: (       8, 4194304) axis:1 max_err:    0.000 max_val: 1806.895 l2_err: 0.00000\n",
      "GBps:1494 ms:  0.101 dim: (       8, 4194304) axis:0 max_err:    0.000 max_val:    6.999 l2_err: 0.00000\n",
      "GBps: 801 ms:  0.168 dim: (      16, 2097152) axis:1 max_err:    0.000 max_val: 1523.933 l2_err: 0.00000\n",
      "GBps:1486 ms:  0.096 dim: (      16, 2097152) axis:0 max_err:    0.000 max_val:   10.317 l2_err: 0.00000\n",
      "GBps: 689 ms:  0.195 dim: (      32, 1048576) axis:1 max_err:    0.000 max_val: 1607.900 l2_err: 0.00000\n",
      "GBps:1485 ms:  0.093 dim: (      32, 1048576) axis:0 max_err:    0.000 max_val:   15.071 l2_err: 0.00000\n",
      "GBps: 522 ms:  0.257 dim: (      64,  524288) axis:1 max_err:    0.000 max_val:  798.253 l2_err: 0.00000\n",
      "GBps:1441 ms:  0.095 dim: (      64,  524288) axis:0 max_err:    0.000 max_val:   20.773 l2_err: 0.00000\n",
      "GBps: 332 ms:  0.405 dim: (     128,  262144) axis:1 max_err:    0.000 max_val:  955.396 l2_err: 0.00000\n",
      "GBps:1294 ms:  0.105 dim: (     128,  262144) axis:0 max_err:    0.000 max_val:   28.347 l2_err: 0.00000\n",
      "GBps: 332 ms:  0.404 dim: (     256,  131072) axis:1 max_err:    0.000 max_val:  743.973 l2_err: 0.00000\n",
      "GBps:1415 ms:  0.095 dim: (     256,  131072) axis:0 max_err:    0.000 max_val:   40.215 l2_err: 0.00000\n",
      "GBps: 332 ms:  0.404 dim: (     512,   65536) axis:1 max_err:    0.000 max_val:  381.389 l2_err: 0.00000\n",
      "GBps:1041 ms:  0.129 dim: (     512,   65536) axis:0 max_err:    0.000 max_val:   52.627 l2_err: 0.00000\n",
      "GBps: 332 ms:  0.405 dim: (    1024,   32768) axis:1 max_err:    0.000 max_val:  308.948 l2_err: 0.00000\n",
      "GBps: 593 ms:  0.227 dim: (    1024,   32768) axis:0 max_err:    0.000 max_val:   75.751 l2_err: 0.00000\n",
      "GBps: 331 ms:  0.406 dim: (    2048,   16384) axis:1 max_err:    0.000 max_val:  226.163 l2_err: 0.00000\n",
      "GBps: 323 ms:  0.416 dim: (    2048,   16384) axis:0 max_err:    0.000 max_val:   94.467 l2_err: 0.00000\n",
      "GBps: 329 ms:  0.408 dim: (    4096,    8192) axis:1 max_err:    0.000 max_val:  168.596 l2_err: 0.00000\n",
      "GBps: 172 ms:  0.783 dim: (    4096,    8192) axis:0 max_err:    0.000 max_val:  137.202 l2_err: 0.00000\n",
      "GBps: 203 ms:  0.661 dim: (    8192,    4096) axis:1 max_err:    0.000 max_val:  138.666 l2_err: 0.00000\n",
      "GBps:  91 ms:  1.483 dim: (    8192,    4096) axis:0 max_err:    0.001 max_val:  193.711 l2_err: 0.00000\n",
      "GBps: 206 ms:  0.651 dim: (   16384,    2048) axis:1 max_err:    0.000 max_val:   95.754 l2_err: 0.00000\n",
      "GBps:  43 ms:  3.145 dim: (   16384,    2048) axis:0 max_err:    0.001 max_val:  264.516 l2_err: 0.00000\n",
      "GBps: 274 ms:  0.490 dim: (   32768,    1024) axis:1 max_err:    0.000 max_val:   71.650 l2_err: 0.00000\n",
      "GBps: 266 ms:  0.505 dim: (   32768,    1024) axis:0 max_err:    0.000 max_val:  381.949 l2_err: 0.00000\n",
      "GBps: 329 ms:  0.408 dim: (   65536,     512) axis:1 max_err:    0.000 max_val:   54.164 l2_err: 0.00000\n",
      "GBps: 265 ms:  0.507 dim: (   65536,     512) axis:0 max_err:    0.000 max_val:  451.445 l2_err: 0.00000\n",
      "GBps: 331 ms:  0.407 dim: (  131072,     256) axis:1 max_err:    0.000 max_val:   40.213 l2_err: 0.00000\n",
      "GBps: 265 ms:  0.507 dim: (  131072,     256) axis:0 max_err:    0.000 max_val:  828.223 l2_err: 0.00000\n",
      "GBps: 332 ms:  0.407 dim: (  262144,     128) axis:1 max_err:    0.000 max_val:   34.650 l2_err: 0.00000\n",
      "GBps: 264 ms:  0.509 dim: (  262144,     128) axis:0 max_err:    0.000 max_val:  718.960 l2_err: 0.00000\n",
      "GBps: 344 ms:  0.396 dim: (  524288,      64) axis:1 max_err:    0.000 max_val:   19.442 l2_err: 0.00000\n",
      "GBps: 290 ms:  0.462 dim: (  524288,      64) axis:0 max_err:    0.000 max_val: 1145.422 l2_err: 0.00000\n",
      "GBps: 350 ms:  0.395 dim: ( 1048576,      32) axis:1 max_err:    0.000 max_val:   15.143 l2_err: 0.00000\n",
      "GBps: 305 ms:  0.439 dim: ( 1048576,      32) axis:0 max_err:    0.000 max_val: 1354.050 l2_err: 0.00000\n",
      "GBps: 702 ms:  0.203 dim: ( 2097152,      16) axis:1 max_err:    0.000 max_val:   10.572 l2_err: 0.00000\n",
      "GBps: 303 ms:  0.443 dim: ( 2097152,      16) axis:0 max_err:    0.000 max_val: 1774.253 l2_err: 0.00000\n",
      "GBps:1368 ms:  0.110 dim: ( 4194304,       8) axis:1 max_err:    0.000 max_val:    7.120 l2_err: 0.00000\n",
      "GBps: 599 ms:  0.224 dim: ( 4194304,       8) axis:0 max_err:    0.000 max_val: 1331.235 l2_err: 0.00000\n",
      "GBps:1527 ms:  0.110 dim: ( 8388608,       4) axis:1 max_err:    0.000 max_val:    3.942 l2_err: 0.00000\n",
      "GBps: 400 ms:  0.336 dim: ( 8388608,       4) axis:0 max_err:    0.000 max_val:  692.203 l2_err: 0.00000\n",
      "GBps:1199 ms:  0.168 dim: (16777216,       2) axis:1 max_err:    0.000 max_val:    1.999 l2_err: 0.00000\n",
      "GBps: 632 ms:  0.212 dim: (16777216,       2) axis:0 max_err:    0.001 max_val: -3031.213 l2_err: 0.00000\n",
      "GBps: 816 ms:  0.329 dim: (33554432,       1) axis:1 max_err:    0.000 max_val:    1.000 l2_err: 0.00000\n",
      "GBps: 635 ms:  0.211 dim: (33554432,       1) axis:0 max_err:    0.001 max_val: -1374.150 l2_err: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# This is a simple kernel used when the number of elements to be reduced in a row/column is small\n",
    "# We simply iterate over the elements to be reduced in a loop and accumulate.\n",
    "\n",
    "def sum_axis_0(Y, X, dim):\n",
    "\n",
    "    code1 = r\"\"\"\n",
    "#define DIM_M 128\n",
    "#define X(i,j) X[(i)*lda+(j)]\n",
    "#define Y(i,j) Y[(i)*lda+(j)]\n",
    "extern \"C\"\n",
    "__global__ void sum_axis_0_k1(float* __restrict__ Y, float* __restrict__ X, const int dim0, const int dim1)\n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int bid = blockIdx.x;\n",
    "    int lda = dim1;\n",
    "    X = &X(0, bid*DIM_M);\n",
    "    Y = &Y(0, bid*DIM_M);\n",
    "    float local_out = 0.0f;\n",
    "    for (int j=0; j<dim0; j++) {\n",
    "      local_out += __ldg(&(X(j, tid)));\n",
    "    }\n",
    "    Y[tid] = local_out;\n",
    "}\n",
    "\"\"\"\n",
    "    # one pass\n",
    "    sig1    = \"PPII\"\n",
    "    DIM_M   = 128\n",
    "    grid1   = (ceil_div(dim[1], DIM_M), 1, 1)\n",
    "    block1  = (min(DIM_M, dim[1]), 1, 1)\n",
    "    params1 = (Y, X, dim[0], dim[1])\n",
    "\n",
    "    func1 = get_kernel(\"sum_axis_0_k1\", code1, sig1, grid1, block1, params1)\n",
    "\n",
    "    return func1\n",
    "\n",
    "def sum_axis_1(Y, X, dim):\n",
    "\n",
    "    # this might require more than 1 kernel (particually if you're avoiding atomics)\n",
    "    code1 = r\"\"\"\n",
    "#define DIM_M 128\n",
    "#define X(i,j) X[(i)*lda+(j)]\n",
    "extern \"C\"\n",
    "__global__ void sum_axis_1_k1(float* __restrict__ Y, float* __restrict__ X, const int dim0, const int dim1)\n",
    "{\n",
    "    int tid = threadIdx.x;\n",
    "    int bid = blockIdx.x;\n",
    "    int lda = dim1;\n",
    "    X = &X(bid*DIM_M, 0);\n",
    "    Y = Y + bid*DIM_M;\n",
    "    float local_out = 0.0f;\n",
    "    for (int j=0; j<dim1; j++) {\n",
    "      local_out += __ldg(&X(tid, j));\n",
    "    }\n",
    "    Y[tid] = local_out;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "    # one pass\n",
    "    sig1    = \"PPII\"\n",
    "    DIM_M   = 128\n",
    "    grid1   = (ceil_div(dim[0], DIM_M), 1, 1)\n",
    "    block1  = (min(DIM_M, dim[0]), 1, 1)\n",
    "    params1 = (Y, X, dim[0], dim[1])\n",
    "\n",
    "    func1 = get_kernel(\"sum_axis_1_k1\", code1, sig1, grid1, block1, params1)\n",
    "\n",
    "    return func1\n",
    "\n",
    "\n",
    "# When the number of elements to be reduced per row/column is relatively large (> 16k), do classic parallel reduction performed in multiple columns/rows in parallel (y dim)\n",
    "\n",
    "def sum_axis_0a(Y, X, dim):\n",
    "    code1 = r\"\"\"\n",
    "// local reduction within a warp\n",
    "__inline__ __device__ float warp_reduce_axis_0(float val) {\n",
    "  for (int offset = warpSize/2; offset > 0; offset /= 2)\n",
    "    val += __shfl_down_sync(0xffffffff, val, offset);\n",
    "  return val;\n",
    "}\n",
    "// reduction within a block\n",
    "__inline__ __device__ float block_reduce_axis_0(float val) {\n",
    "  static __shared__ float shared[32];\n",
    "  int lane = threadIdx.x % warpSize;\n",
    "  int wid = threadIdx.x / warpSize;\n",
    "  val = warp_reduce_axis_0(val);\n",
    "  if (!lane) shared[wid]=val;\n",
    "  __syncthreads();\n",
    "  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n",
    "  if (!wid) val = warp_reduce_axis_0(val);\n",
    "  return val;\n",
    "}\n",
    "// grid reduce\n",
    "extern \"C\"\n",
    "__global__ void sum_axis_0_grid_reduce(float* __restrict__ Y, float* __restrict__ X, const int dim0, const int dim1)\n",
    "{\n",
    "  float sum = 0.0f;\n",
    "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i<dim1; i += blockDim.x * gridDim.x) {\n",
    "    sum += __ldg(&X[dim0*i+blockIdx.y]);\n",
    "  }\n",
    "  sum = block_reduce_axis_0(sum);\n",
    "  if (!threadIdx.x) {\n",
    "    Y[blockIdx.y+dim0*blockIdx.x] = sum;\n",
    "  }\n",
    "}\n",
    "// grid reduce variant which improves memory access pattern slightly\n",
    "extern \"C\"\n",
    "__global__ void sum_axis_0_grid_reduce_4(float* __restrict__ Y, float* __restrict__ X, const int dim0, const int dim1)\n",
    "{\n",
    "  float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n",
    "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i<dim1; i += blockDim.x * gridDim.x) {\n",
    "    #pragma unroll\n",
    "    for (int j=0; j<4; j++) {\n",
    "      sum[j] += __ldg(&X[dim0*i+(4*blockIdx.y+j)]);\n",
    "    }\n",
    "  }\n",
    "  #pragma unroll 4\n",
    "  for (int j=0; j<4; j++) {\n",
    "    sum[j] = block_reduce_axis_0(sum[j]);\n",
    "  }\n",
    "  if (!threadIdx.x) {\n",
    "    #pragma unroll 4\n",
    "    for (int j=0; j<4; j++) {\n",
    "      Y[(4*blockIdx.y+j)+dim0*blockIdx.x] = sum[j];\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "    P = drv.mem_alloc(dim[1] * 1024 * 4)\n",
    "\n",
    "    sig1    = \"PPII\"\n",
    "    threads = 256\n",
    "    blocks = min(ceil_div(dim[0], threads), 1024)\n",
    "    grid1   = (blocks, dim[1], 1)\n",
    "    grid2   = (1, dim[1], 1)\n",
    "    grid1_4   = (blocks, dim[1]//4, 1)\n",
    "    grid2_4   = (1, dim[1]//4, 1)\n",
    "    block1  = (threads, 1, 1)\n",
    "    block2  = (blocks, 1, 1)\n",
    "\n",
    "    # one pass ok\n",
    "    if blocks == 1:\n",
    "      P = Y\n",
    "\n",
    "    params1 = (P, X, dim[1], dim[0])\n",
    "    params2 = (Y, P, dim[1], blocks)\n",
    "\n",
    "    func0a = get_kernel(\"sum_axis_0_grid_reduce\", code1, sig1, grid1, block1, params1)\n",
    "    func0a4 = get_kernel(\"sum_axis_0_grid_reduce_4\", code1, sig1, grid1_4, block1, params1)\n",
    "    func0b = get_kernel(\"sum_axis_0_grid_reduce\", code1, sig1, grid2, block2, params2)\n",
    "    func0b4 = get_kernel(\"sum_axis_0_grid_reduce_4\", code1, sig1, grid2_4, block2, params2)\n",
    "\n",
    "    def func_0():\n",
    "      # some heuristic for choosing a slightly faster kernel\n",
    "      func0a() if dim[1] < 8 else func0a4()\n",
    "\n",
    "      # 2 passes needed\n",
    "      if blocks > 1:\n",
    "        func0b() if dim[1] < 8 else func0b4()\n",
    "\n",
    "    return func_0\n",
    "\n",
    "# same algorithm for slighly modified for axis 1\n",
    "\n",
    "def sum_axis_1a(Y, X, dim):\n",
    "    code1 = r\"\"\"\n",
    "__inline__ __device__ float warp_reduce_axis_1(float val) {\n",
    "  for (int offset = warpSize/2; offset > 0; offset /= 2)\n",
    "    val += __shfl_down_sync(0xffffffff, val, offset);\n",
    "  return val;\n",
    "}\n",
    "__inline__ __device__ float block_reduce_axis_1(float val) {\n",
    "  static __shared__ float shared[32];\n",
    "  int lane = threadIdx.x % warpSize;\n",
    "  int wid = threadIdx.x / warpSize;\n",
    "  val = warp_reduce_axis_1(val);\n",
    "  if (!lane) shared[wid]=val;\n",
    "  __syncthreads();\n",
    "  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n",
    "  if (!wid) val = warp_reduce_axis_1(val);\n",
    "  return val;\n",
    "}\n",
    "extern \"C\"\n",
    "__global__ void sum_axis_1_grid_reduce(float* __restrict__ Y, float* __restrict__ X, const int dim0, const int dim1)\n",
    "{\n",
    "  float sum = 0.0f;\n",
    "  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i<dim1; i += blockDim.x * gridDim.x) {\n",
    "    sum += __ldg(&X[dim1*blockIdx.y+i]);\n",
    "  }\n",
    "  sum = block_reduce_axis_1(sum);\n",
    "  if (!threadIdx.x) {\n",
    "    Y[blockIdx.y*gridDim.x+blockIdx.x] = sum;\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "    P = drv.mem_alloc(dim[0] * 1024 * 4)\n",
    "\n",
    "    sig1    = \"PPII\"\n",
    "    threads = 256\n",
    "    blocks = min(ceil_div(dim[1], threads), 1024)\n",
    "    grid1   = (blocks, dim[0], 1)\n",
    "    grid2   = (1, dim[0], 1)\n",
    "    block1  = (threads, 1, 1)\n",
    "    block2  = (blocks, 1, 1)\n",
    "    params1 = (P, X, dim[0], dim[1])\n",
    "    params2 = (Y, P, dim[0], blocks)\n",
    "\n",
    "    func1a = get_kernel(\"sum_axis_1_grid_reduce\", code1, sig1, grid1, block1, params1)\n",
    "    func1b = get_kernel(\"sum_axis_1_grid_reduce\", code1, sig1, grid2, block2, params2)\n",
    "\n",
    "    def func_1():\n",
    "      func1a()\n",
    "\n",
    "      # 2 passes needed\n",
    "      if blocks > 1:\n",
    "        func1b()\n",
    "\n",
    "    return func_1\n",
    "\n",
    "####################\n",
    "# Helper code\n",
    "\n",
    "def get_kernel(name, code, sig, grid, block, params):\n",
    "\n",
    "    # you might find it easier to develop the cuda code in separate files\n",
    "    if code[-3:] == \".cu\":\n",
    "        with open(code) as file:\n",
    "            code = file.read()\n",
    "\n",
    "    module = SourceModule(code, include_dirs=[file_path], no_extern_c=True)\n",
    "    kernel = module.get_function(name)\n",
    "    kernel.prepare(sig)\n",
    "\n",
    "    def func():\n",
    "        kernel.prepared_call(grid, block, *params)\n",
    "\n",
    "    return func\n",
    "\n",
    "# Potentially handy\n",
    "def ceil_div(x, y):\n",
    "    return -(-x // y)\n",
    "\n",
    "def test_reduce(dim, axis, repeat=1, ones=False, out=False):\n",
    "\n",
    "    if ones:\n",
    "        # possibly helpful for debugging\n",
    "        x = np.ones(dim, dtype=np.float32)\n",
    "    else:\n",
    "        x = np.random.uniform(-1.0, 1.0, dim).astype(np.float32)\n",
    "\n",
    "    y = np.empty(dim[1-axis], dtype=np.float32)\n",
    "\n",
    "    X = drv.mem_alloc(x.nbytes)\n",
    "    Y = drv.mem_alloc(y.nbytes)\n",
    "    drv.memcpy_htod(X, x)\n",
    "\n",
    "    # some heuristic to select a kernel based on the dimension\n",
    "    # this needs more tuning\n",
    "    if axis == 1:\n",
    "      kernel_func = sum_axis_1a if dim[1] >= 8192 else sum_axis_1\n",
    "    else:\n",
    "      kernel_func = sum_axis_0a if dim[0] >= 32768 else sum_axis_0\n",
    "\n",
    "    gpu_func = kernel_func(Y, X, dim)\n",
    "\n",
    "    start = drv.Event()\n",
    "    end   = drv.Event()\n",
    "\n",
    "    for r in range(repeat):\n",
    "        gpu_func() # warm up the clock\n",
    "\n",
    "    start.record()\n",
    "    for r in range(repeat):\n",
    "        gpu_func()\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "\n",
    "    # compute the approximate memory bandwidth\n",
    "    nbytes = y.nbytes + x.nbytes\n",
    "    ms     = end.time_since(start) / repeat\n",
    "    gbps   = nbytes / (ms * 1e6)\n",
    "\n",
    "    # get result and compare to numpy calculation\n",
    "    # Compute the numpy sum in float64 for the highest accuracy baseline.\n",
    "    # There's no need to use float64 inside your kernels, but accuracy is still an important metric.\n",
    "    drv.memcpy_dtoh(y, Y)\n",
    "    z = np.sum(x.astype(np.float64), axis=axis).astype(np.float32)\n",
    "    d = np.abs(y - z)\n",
    "\n",
    "    l2_err = np.sqrt(np.square(d).sum()) / np.sqrt(np.square(z).sum())\n",
    "\n",
    "    print(\"GBps:%4.0f ms:%7.3f dim: (%8d,%8d) axis:%d max_err: %8.3f max_val: %8.3f l2_err: %7.5f\" % (\n",
    "        gbps, ms, dim[0], dim[1], axis, d.max(), y.max(), l2_err ))\n",
    "\n",
    "    if out:\n",
    "        # inspect your output to debug\n",
    "        fmt = \"%5.0f\" if ones else \"%5.2f\"\n",
    "        #np.savetxt(\"out_dif.txt\", d, fmt=fmt)\n",
    "        ##np.savetxt(\"out_cpu.txt\", z, fmt=fmt)\n",
    "        #np.savetxt(\"out_gpu.txt\", y, fmt=fmt)\n",
    "        print(ms, nbytes, gbps, x.shape, z.shape)\n",
    "        exit()\n",
    "\n",
    "####################\n",
    "# Execution code\n",
    "\n",
    "ones   = 0\n",
    "out    = 0\n",
    "repeat = 10 # use a large repeat count to measure bandwidth more accurately\n",
    "\n",
    "# feel free to play with this to see how robust your performance is to different shapes\n",
    "for exp0 in range(26):\n",
    "    exp1 = 26 - exp0 - 1\n",
    "    dim0 = 2 ** exp0\n",
    "    dim1 = 2 ** exp1\n",
    "\n",
    "    # reduce axis=1\n",
    "    test_reduce((dim0, dim1), axis=1, repeat=repeat, ones=ones, out=out)\n",
    "    # reduce axis=0\n",
    "    test_reduce((dim0, dim1), axis=0, repeat=repeat, ones=ones, out=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bd262-4668-470d-908c-d7738a478e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
